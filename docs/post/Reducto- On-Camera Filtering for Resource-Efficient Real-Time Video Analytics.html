<!DOCTYPE html>
<html data-color-mode="light" data-dark-theme="dark" data-light-theme="light" lang="zh-CN">
<head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />
    
    <link rel="icon" href="https://github.githubassets.com/favicons/favicon.svg"><script>
        let theme = localStorage.getItem("meek_theme") || "light";
        document.documentElement.setAttribute("data-color-mode", theme);
    </script>
<meta name="description" content="第一作者（共一）：Yuanqi Li,   Arthi Padmanabhan

学校：加利福尼亚大学洛杉矶分校（University of California, Los Angeles）

## 摘要

提出了Reducto，一个在摄像头端进行数据过滤并且可以根据各种特征进行动态自适应调整的决策系统（任务仅限识别，计数，检测）

## 背景介绍

#### 简单概括

由于相机性能抛弃压缩检测模型方法，由于速度和过滤方式抛弃二元分类模型并旨在帧差分方法上特征间的关系寻找动态阈值来实现更好的效果。">
<meta property="og:title" content="Reducto: On-Camera Filtering for Resource-Efficient Real-Time Video Analytics">
<meta property="og:description" content="第一作者（共一）：Yuanqi Li,   Arthi Padmanabhan

学校：加利福尼亚大学洛杉矶分校（University of California, Los Angeles）

## 摘要

提出了Reducto，一个在摄像头端进行数据过滤并且可以根据各种特征进行动态自适应调整的决策系统（任务仅限识别，计数，检测）

## 背景介绍

#### 简单概括

由于相机性能抛弃压缩检测模型方法，由于速度和过滤方式抛弃二元分类模型并旨在帧差分方法上特征间的关系寻找动态阈值来实现更好的效果。">
<meta property="og:type" content="article">
<meta property="og:url" content="https://nexor-source.github.io/post/Reducto-%20On-Camera%20Filtering%20for%20Resource-Efficient%20Real-Time%20Video%20Analytics.html">
<meta property="og:image" content="https://github.githubassets.com/favicons/favicon.svg">
<title>Reducto: On-Camera Filtering for Resource-Efficient Real-Time Video Analytics</title>



</head>
<style>
body{box-sizing: border-box;min-width: 200px;max-width: 900px;margin: 20px auto;padding: 45px;font-size: 16px;font-family: sans-serif;line-height: 1.25;}
#header{display:flex;padding-bottom:8px;border-bottom: 1px solid var(--borderColor-muted, var(--color-border-muted));margin-bottom: 16px;}
#footer {margin-top:64px; text-align: center;font-size: small;}

</style>

<style>
.postTitle{margin: auto 0;font-size:40px;font-weight:bold;}
.title-right{display:flex;margin:auto 0 0 auto;}
.title-right .circle{padding: 14px 16px;margin-right:8px;}
#postBody{border-bottom: 1px solid var(--color-border-default);padding-bottom:36px;}
#postBody hr{height:2px;}
#cmButton{height:48px;margin-top:48px;}
#comments{margin-top:64px;}
.g-emoji{font-size:24px;}
@media (max-width: 600px) {
    body {padding: 8px;}
    .postTitle{font-size:24px;}
}

</style>




<body>
    <div id="header">
<h1 class="postTitle">Reducto: On-Camera Filtering for Resource-Efficient Real-Time Video Analytics</h1>
<div class="title-right">
    <a href="https://nexor-source.github.io" id="buttonHome" class="btn btn-invisible circle" title="首页">
        <svg class="octicon" width="16" height="16">
            <path id="pathHome" fill-rule="evenodd"></path>
        </svg>
    </a>
    
    <a href="https://github.com/nexor-source/nexor-source.github.io/issues/4" target="_blank" class="btn btn-invisible circle" title="Issue">
        <svg class="octicon" width="16" height="16">
            <path id="pathIssue" fill-rule="evenodd"></path>
        </svg>
    </a>
    

    <a class="btn btn-invisible circle" onclick="modeSwitch();" title="切换主题">
        <svg class="octicon" width="16" height="16" >
            <path id="themeSwitch" fill-rule="evenodd"></path>
        </svg>
    </a>

</div>
</div>
    <div id="content">
<div class="markdown-body" id="postBody"><p>第一作者（共一）：Yuanqi Li,   Arthi Padmanabhan</p>
<p>学校：加利福尼亚大学洛杉矶分校（University of California, Los Angeles）</p>
<h2>摘要</h2>
<p>提出了Reducto，一个在摄像头端进行数据过滤并且可以根据各种特征进行动态自适应调整的决策系统（任务仅限识别，计数，检测）</p>
<h2>背景介绍</h2>
<h4>简单概括</h4>
<p>由于相机性能抛弃压缩检测模型方法，由于速度和过滤方式抛弃二元分类模型并旨在帧差分方法上特征间的关系寻找动态阈值来实现更好的效果。</p>
<h4>现状分析与常用方法对比</h4>
<p>摄像头在城市中的覆盖范围逐年增大，神经网络方法逐年发展，导致越来越多的视频源需要进行各种各样的处理。<strong>摄像机拍摄到视频后会传输到云服务器（或先传输到边缘设备），云服务器立刻运行检测模型来完成用户相关的视频查询</strong>。实时视频分析Pipeline中由于<strong>需要大量的网络和计算资源</strong>，需要在系统中有针对视频的<strong>过滤机制</strong>，目前系统采用的过滤方法常见的有：</p>
<ol>
<li>[时间冗余消除] 使用<strong>更小的压缩检测模型</strong>（比如Tiny YOLO，Focus）先生成置信度，再根据置信度来判断帧要不要发送到完整模型。其运行速度最慢，甚至难以达到实时输出。</li>
<li>[时间冗余消除] 专门的<strong>二元分类模型</strong>（比如 FilterForward NoScope），对每个帧进行判断看是或不是包含兴趣对象的帧，不是就抛弃。其运行速度中等，在合适的配置下可以实现实时输出。</li>
<li>[时间冗余消除] 简单<strong>帧差分方法</strong>（比如Glimpse），通过比较视频的低级像素特征上的变化来进行时间维度冗余帧的消除。其运行速度较快，计算较为简单，基本在各种配置下都可轻松实现实时输出。</li>
</ol>
<p>大部分的过滤会放到云边端模型上的边缘侧或者云侧来进行过滤，作者希望将过滤的操作放到相机端，这样<strong>尽早过滤数据可以减轻后续的计算负担和网络负担</strong>。但由于商用相机的计算资源很有限，即便是之前提到的压缩后的神经网络也在摄像头端排除在外（CPU1GHz和RAM 256MB使得Tiny YOLO的运行速度＜1FPS），而二元分类NN常常保留过多冗余，帧差分常常过滤太多帧导致准确性下降。</p>
<h4>为什么要拓展帧差分方法</h4>
<p>和离线最优策略（离线计算所有帧并且进行最优过滤）相比，<strong>二元分类会多放70%+的数据</strong>（比如现在任务是对车辆计数，场景基本静态，这里由于每帧都有兴趣对象所以二元分类会放很多帧，而实际上极多帧间的车辆数量都没有发生任何变化，存在巨大的过滤空间）</p>
<p>帧差分过滤方法之所以效果不好是因为视频的低级特征并没有被完全合理的使用，比如 Glimpse 方法结合像素级的帧差异和<strong>静态的阈值，但其泛化性并不好</strong>，这是因为不同的场景里相同的像素差值代表着不同的含义。因此如果作者能够<strong>构建出特征类型，阈值和检测准确度要求这三者之间的关系</strong>，或许就能取得比二元分类更好的效果。</p>
<p>因此作者设计了关于视频低级特征的几个机制：</p>
<ul>
<li>
<p>Reducto 服务器对历史视频数据进行离线分析，寻找每个查询类别对应的最佳特征</p>
</li>
<li>
<p>摄像头端根据视频内容计算完成的每秒变化的差分阈值（使用轻量级机器学习）</p>
<p>对于某个query，server指定需要关注的特征，为了确定过滤特征使用的阈值，使用大量历史成对数据（针对某query，该特征的阈值设定为 x 时取得的检测准确率 y），抛弃所有达不到检测准确率的成对数据，。。。。。。</p>
</li>
<li>
<p>对于视频特征变化剧烈的情况，预测模型可能表现不佳，因为准确度不在相机上进行计算，所以相机端只对特征进行判断，<strong>如果特征没有落入之前的所有簇中</strong>，则说明特征偏移较大，此时相机<strong>停止过滤并且向服务器发送信号来在线重新训练模型</strong>，模型训练结束后传回相机（<em>这段时间中相机还会使用过滤策略吗？</em>）</p>
</li>
</ul>
<p>因此作者构建了Reducto，它可以根据特征，查询准确性，视频内容的变化进行动态调整过滤决策。作者使用实时交通监控摄像头的视频来进行对比评估，任务选择的是针对人和车的目标识别，检测，计数任务。并且相比FilterForward和Chameleon取得了更好的效果。</p>
<h4>相机性能对算法的影响</h4>
<p><strong>最新最先进的智能相机：</strong></p>
<p>近年来的智能相机甚至内置四核处理器（Ambarella）和小型GPU （DNNCam）从而可以轻松处理基于CNN/DNN的算法，对于这种相机可以直接将基于NN的过滤算法放置在相机侧。</p>
<p><strong>商用/部署摄像头：</strong></p>
<p>大规模商用摄像头成本大约20-100美元，带有一定的计算资源（单CPU无GPU）。并且考虑到规模，这些摄像头的更新换代速度较为缓慢，因此非常有必要考虑基于摄像头基础资源的过滤算法（这里假定摄像头都是具有一定的计算资源的）</p>
<h2>视频特征的选取</h2>
<p>使用CV社区的差分特征列表，根据特征计算量进行预分组，通过选择：</p>
<ul>
<li>满足实时性</li>
<li>差分变化和各种任务的查询结果的变化高度相关</li>
</ul>
<p>的特征来测试。</p>
<h4>低级特征：</h4>
<ul>
<li>像素差异（pixel difference）：通过逐个像素比对亮度值或者RGB三通道分别计算差值来得到差异，计算简单但是噪声较大。</li>
<li>边缘差异（edge difference）：通过Canny或者Sobel算子来计算梯度边缘信息，通过比对边缘图的像素差异或者别的计算方法来得到差异值，是结构性的差异可以一定程度上减少光照的影响，但噪声仍然可以制造出一些伪边缘来影响结果。</li>
</ul>
<h4>高级特征：</h4>
<ul>
<li>尺度不变特征变换（SIFT）：根据算法提取关键点，并且针对关键点生成尺度和旋转不变的描述子，具有更高的抗噪性但计算开销大</li>
<li>加速鲁棒特征（SURF）：同样也是提取特征点并且对特征点进行描述，但相比SIFT计算量稍小一些</li>
</ul>
<h4>低级特征的必要性和合理性</h4>
<p>不同特征的检测速度对照表如下，</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/user-attachments/assets/9714c857-73df-4c4a-a2a8-090b4f2cd576"><img src="https://github.com/user-attachments/assets/9714c857-73df-4c4a-a2a8-090b4f2cd576" alt="image-20241208163352080" style="max-width: 100%;"></a></p>
<p>作者发现在这样配置的相机上使用高级特征进行检测很难达到实时的速度，所以还是得使用低级特征来进行拓展（不包括角点检测因为角点低级特征的摄像头端检测速度也不够实时）</p>
<p>且作者还发现无论是在计数任务还是边界框任务中，Pixel、Edge 和 Area 特征的变化值都与查询结果的变化呈现出强相关性，这表明即使特征值本身存在一定噪声，使用这些特征仍能够在实时系统中有效预测查询结果的变化。</p>
<h2>系统架构和工作流</h2>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/user-attachments/assets/8bfc513c-8023-4b6c-88fc-e9118b163eee"><img src="https://github.com/user-attachments/assets/8bfc513c-8023-4b6c-88fc-e9118b163eee" alt="image-20241208164636352" style="max-width: 100%;"></a></p>
<ol>
<li>离线profiler对几分钟的视频进行深入分析（检测目标，并且计算所有特征的差异值），寻找最适合每个查询类别的特征
<ul>
<li>作者发现通常某个单个特征可以作为某类查询在各种不同相机和视频以及准确度要求下的最优特征</li>
<li><em>profiler会将每一段视频的每一个特征都聚合在一起进行比对，在满足查询精度的情况下使用过滤最多帧的特征和对应的阈值（这里profiler是使用的历史hash table来得到的阈值吗，聚合仅仅是把所有特征放在一起比对吗）</em></li>
</ul>
</li>
<li><strong>确定当前场景最佳特征</strong>：当用户指定查询目标和精度之后，server将最佳特征通知给相机，相机端开始准备传输帧。首先差异提取器Diff Extractor会计算server给出的最佳特征</li>
<li><strong>构建hash table确定阈值</strong>：针对每一种查询特征，server都会用model trainer来训练一个确定阈值的简单回归模型，通过摄像头发来的前几帧数据提取特征并且使用K-means进行聚类，得到一个哈希表（hash table）key为聚类的类中的平均差异距离，value为对应的最佳阈值。server会把这个hash table发送到camera端用于查询过滤。构建Hash Table的详细过程见下文专门的章节。<del>（这里hash table用来确定最佳阈值的做法完全没有看懂，为什么要对前几帧进行聚类？针对多簇平均差异距离-最佳阈值这种key-value对会出现有高差异值被过滤而第差异值不被过滤的情况，非常反直觉，并且这是使用camera前几帧构建的，也没有涉及到场景的变化或者准确率或者查询特征的变化。那为什么这种情况会训练出来一个hash table这种分段函数而不是一个固定值呢？还有就是在确定每个簇的平均差异距离和对应的最佳阈值的时候是怎么做的呢？）</del></li>
<li><strong>hash table再更新（在线训练）</strong>：差异值距离hash table中的key过远时会将这些不匹配的帧发送给model trainer中添加到数据集中进行更新，由于不同场景（比如晴天和雨天）会导致聚类的结果相距甚远，因此离线的模型肯定不够，需要定时进行在线更新。这些用于训练的帧不会被进行任何过滤，且<em>用户可以选择是否保留model更新前的帧以供重新检测？</em></li>
<li><strong>不同粒度的确定</strong>：由于hash table使用的key是多帧的平均差异距离，因此在选择视频片段长度时过小的视频段延迟更低，但是受到的噪声影响更大，更大的视频段延迟更高，但是不易受到噪声影响。（<em>这里的视频段中的每个帧还需要走K-means聚类算法吗？</em>）最终凭经验作者使用每段时长1S。</li>
</ol>
<h4>最佳特征和查询任务的关系</h4>
<p>作者发现最佳特征在任务间经常变动，但对于某查询任务，不同视频或者不同目标精度基本不会影响最佳特征的变动，并且<strong>Area低级特征适合计数类的查询，Pixel和Edge低级特征适合边界框类的查询。</strong></p>
<h4>Hash Table构建细节</h4>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/user-attachments/assets/9e243d78-8c1b-4983-a579-ecf6f42cb151"><img src="https://github.com/user-attachments/assets/9e243d78-8c1b-4983-a579-ecf6f42cb151" alt="image-20241209092319868" style="max-width: 100%;"></a></p>
<p>由于最佳阈值随时间抖动幅度和频率都很高，因此需要动态调整特征阈值，构建Hash Table。</p>
<ol>
<li>用户给定查询任务，acc要求</li>
<li>初期收集未过滤的帧来进行训练，假设每段视频是1s，30fps，也就是一共30帧，那么就可以构建得到3个29维的向量，每一维代表两帧之间的特征差异值，一共三种特征每种都构建一个。</li>
<li>遍历预设定的候选阈值计算得到每个向量在每个特征的每个候选阈值下的查询acc，如果达不到acc要求就删除。</li>
<li>留下的向量会使用K-means进行聚类（类数=5），每个data point是一个向量，向量之间衡量距离的函数作者并没有详细给出。</li>
<li>最终每个簇内就可以找到满足所有acc要求的最合适的阈值（尽可能大，这样差异值小于阈值的帧就可以尽可能过滤从而节省更多空间）</li>
<li>Hash Table生成的Key是一个二元组（$V_{center}, V_{variance}$），$V_{center}$代表簇内所有向量的平均，$V_{variance}$的每个元素代表簇中任意两个向量的这个元素上最大的距离，也就是代表散度。</li>
<li>构建好Hash Table之后camera端之后输入的视频就会去构建向量，聚类，并且计算$V_{center}, V_{variance}$从而在Hash Table中进行查询对应的最佳阈值（如果距离Hash Table中的所有Key都超过了预定距离则会触发Hash Table的再更新。</li>
<li>得到阈值后过滤阈值以下的微小变化帧，将剩余的帧使用.264编码压缩发送到server。</li>
</ol>
<h2>评估</h2>
<h4>数据集</h4>
<p>使用北美各地部署的7个实时监控摄像头的公开视频流收集了25个10min长度的视频，包含全天时段，光照，天气，人/汽车的不同密度</p>
<h4>查询任务</h4>
<ul>
<li>Tagging：返回帧中是否包含兴趣物体的二元分类任务</li>
<li>Counting（计数）：返回帧内兴趣物体数量的任务</li>
<li>Bounding Box Detection：返回帧中所有检测到的兴趣对象的边界框坐标，准确度使用mAP指标衡量（类似交并比）</li>
</ul>
<h4>配置</h4>
<p>GT使用YOLO进行计算，server配置8核CPU和Tesla V100，相机配置VM或树莓派，VM使用了256MB内存和1GHz CPU</p>
<h4>baseline</h4>
<ul>
<li>完全不过滤的检测</li>
<li>Reducto（Offline）完美过滤所有不会降低ACC的帧</li>
<li>Tiny YOLO（压缩检测NN模型）</li>
<li>FilterForward（二元分类模型）</li>
<li>Cloudseg（超分辨还原）</li>
<li>Chameleon（动态调整采样视频质量）</li>
</ul>
<p>相比之下Reducto很优越，具体对比可见论文</p>
<h2>感想</h2>
<ol>
<li><strong>低级特征使用的还是太少了，涵盖面太窄</strong>，在非常多变的环境下我认为不具备参考意义，感觉相比同类的论文，轻量级的特征提取NN会拥有更好的效果并且具有更高的可扩展性，因为NN可以提取更多的信息，包括运动姿势等，从而实现检测任务的扩展。当然前提是轻量级的NN足够轻量。相比之下这种只基于Area/Edge/Pixel差异值的判断再加上复杂的逻辑规则来挖掘潜力的方法看上去并不好搞，而且想要超越别人的方法更是困难。</li>
<li>Reducto的过滤规则仍然是基于序列时序上特征差值的过滤，这种时序的过滤主要针对时间冗余进行处理，而<strong>对于空间冗余基本没有涉及</strong>，这也是后面论文对Reducto方法所诟病的地方之一。</li>
</ol></div>
<div style="font-size:small;margin-top:8px;float:right;"></div>

<button class="btn btn-block" type="button" onclick="openComments()" id="cmButton">评论</button>
<div class="comments" id="comments"></div>

</div>
    <div id="footer"><div id="footer1">Copyright © <span id="copyrightYear"></span> <a href="https://nexor-source.github.io">马越's Blog</a></div>
<div id="footer2">
    <span id="runday"></span><span>Powered by <a href="https://meekdai.com/Gmeek.html" target="_blank">Gmeek</a></span>
</div>

<script>
var now=new Date();
document.getElementById("copyrightYear").innerHTML=now.getFullYear();

if(""!=""){
    var startSite=new Date("");
    var diff=now.getTime()-startSite.getTime();
    var diffDay=Math.floor(diff/(1000*60*60*24));
    document.getElementById("runday").innerHTML="网站运行"+diffDay+"天"+" • ";
}
</script></div>
</body>
<script>
var IconList={'sun': 'M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z', 'moon': 'M9.598 1.591a.75.75 0 01.785-.175 7 7 0 11-8.967 8.967.75.75 0 01.961-.96 5.5 5.5 0 007.046-7.046.75.75 0 01.175-.786zm1.616 1.945a7 7 0 01-7.678 7.678 5.5 5.5 0 107.678-7.678z', 'sync': 'M1.705 8.005a.75.75 0 0 1 .834.656 5.5 5.5 0 0 0 9.592 2.97l-1.204-1.204a.25.25 0 0 1 .177-.427h3.646a.25.25 0 0 1 .25.25v3.646a.25.25 0 0 1-.427.177l-1.38-1.38A7.002 7.002 0 0 1 1.05 8.84a.75.75 0 0 1 .656-.834ZM8 2.5a5.487 5.487 0 0 0-4.131 1.869l1.204 1.204A.25.25 0 0 1 4.896 6H1.25A.25.25 0 0 1 1 5.75V2.104a.25.25 0 0 1 .427-.177l1.38 1.38A7.002 7.002 0 0 1 14.95 7.16a.75.75 0 0 1-1.49.178A5.5 5.5 0 0 0 8 2.5Z', 'home': 'M6.906.664a1.749 1.749 0 0 1 2.187 0l5.25 4.2c.415.332.657.835.657 1.367v7.019A1.75 1.75 0 0 1 13.25 15h-3.5a.75.75 0 0 1-.75-.75V9H7v5.25a.75.75 0 0 1-.75.75h-3.5A1.75 1.75 0 0 1 1 13.25V6.23c0-.531.242-1.034.657-1.366l5.25-4.2Zm1.25 1.171a.25.25 0 0 0-.312 0l-5.25 4.2a.25.25 0 0 0-.094.196v7.019c0 .138.112.25.25.25H5.5V8.25a.75.75 0 0 1 .75-.75h3.5a.75.75 0 0 1 .75.75v5.25h2.75a.25.25 0 0 0 .25-.25V6.23a.25.25 0 0 0-.094-.195Z', 'github': 'M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z'};
var utterancesLoad=0;

let themeSettings={
    "dark": ["dark","moon","#00f0ff","dark-blue"],
    "light": ["light","sun","#ff5000","github-light"],
    "auto": ["auto","sync","","preferred-color-scheme"]
};
function changeTheme(mode, icon, color, utheme){
    document.documentElement.setAttribute("data-color-mode",mode);
    document.getElementById("themeSwitch").setAttribute("d",value=IconList[icon]);
    document.getElementById("themeSwitch").parentNode.style.color=color;
    if(utterancesLoad==1){utterancesTheme(utheme);}
}
function modeSwitch(){
    let currentMode=document.documentElement.getAttribute('data-color-mode');
    let newMode = currentMode === "light" ? "dark" : currentMode === "dark" ? "auto" : "light";
    localStorage.setItem("meek_theme", newMode);
    if(themeSettings[newMode]){
        changeTheme(...themeSettings[newMode]);
    }
}
function utterancesTheme(theme){
    const message={type:'set-theme',theme: theme};
    const iframe=document.getElementsByClassName('utterances-frame')[0];
    iframe.contentWindow.postMessage(message,'https://utteranc.es');
}
if(themeSettings[theme]){changeTheme(...themeSettings[theme]);}
console.log("\n %c Gmeek last https://github.com/Meekdai/Gmeek \n","padding:5px 0;background:#02d81d;color:#fff");
</script>

<script>
document.getElementById("pathHome").setAttribute("d",IconList["home"]);
document.getElementById("pathIssue").setAttribute("d",IconList["github"]);



function openComments(){
    cm=document.getElementById("comments");
    cmButton=document.getElementById("cmButton");
    cmButton.innerHTML="loading";
    span=document.createElement("span");
    span.setAttribute("class","AnimatedEllipsis");
    cmButton.appendChild(span);

    script=document.createElement("script");
    script.setAttribute("src","https://utteranc.es/client.js");
    script.setAttribute("repo","nexor-source/nexor-source.github.io");
    script.setAttribute("issue-term","title");
    
    if(localStorage.getItem("meek_theme")=="dark"){script.setAttribute("theme","dark-blue");}
    else if(localStorage.getItem("meek_theme")=="light") {script.setAttribute("theme","github-light");}
    else{script.setAttribute("theme","preferred-color-scheme");}
    
    script.setAttribute("crossorigin","anonymous");
    script.setAttribute("async","");
    cm.appendChild(script);

    int=self.setInterval("iFrameLoading()",200);
}

function iFrameLoading(){
    var utterances=document.getElementsByClassName('utterances');
    if(utterances.length==1){
        if(utterances[0].style.height!=""){
            utterancesLoad=1;
            int=window.clearInterval(int);
            document.getElementById("cmButton").style.display="none";
            console.log("utterances Load OK");
        }
    }
}



</script>


</html>
